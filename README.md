# Generating altered logos
manip.py will look into LOGO_DIR for unaltered logos. The file names should be in the form ``[LOGO_NAME].png``.

Manipulated logos will be generated and put into the same LOGO_DIR. The generated file names will be in the form ``[LOGO_NAME].[MANIPULATION_TYPE][MANIPULATION_STRENGTH].png``.

prompt.py expects both unaltered logo files and altered logo files to be in this format.

# Classifying Logos
prompt.py will look into the LOGO_DIR folder and cross check with the csv at DB_PATH to see if it has been classified yet. If not, it will prompt for classification and record the results to the db.

# Todo
There should be another image module that converts all the images to PNG. PNG generally holds more information than JPG, including an alpha (transparency) layer. All files are currently being converted to RGB, without the transparency layer, so logos with transparency have their transparent parts replaced with opaque gray. We don't want that.

We need to attempt adversarial attacks, where we inject specifically crafted minimal noise that causes the network to misclassify.

# Adversarial attacks
For LLama, the model itself is given, so maybe we can find a way to train the model itself to generate the images.

GPT and Gemini likely have no hope.
But we could try black box attacks (which GPT could explain more about).
The only concern with black box attacks is it will be incredibly limited by the API rate-limits.

# What we are not doing
We are not doing font changes nor are we doing AI manipulated images using DALLE or MidJourney.

# Main hopes
Hopefully, we can atleast finish classifying logos generated by the ``random noise`` and ``saturation shifting``, but I'm not actually expecting them to fool the models at all (based on initial tests).

If we actually want to trick the models, it will be through adversarial noise.
So adversarial noise will be the main challenge.